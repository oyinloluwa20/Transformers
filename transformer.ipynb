{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 166,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'2.0.1+cpu'"
                        ]
                    },
                    "execution_count": 166,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import math\n",
                "torch.__version__"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 167,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cpu\n"
                    ]
                }
            ],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 168,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Hyperparameters\n",
                "num_heads = 8\n",
                "embed_len = 512\n",
                "batch_size = 8\n",
                "stack_len = 6\n",
                "dropout = 0.1\n",
                "\n",
                "output_vocab_size = 7000 #just for testing\n",
                "input_vocab_size = 7000 #just for testing"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bulid the embedding block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 169,
            "metadata": {},
            "outputs": [],
            "source": [
                "class InputEmbedding(nn.Module):\n",
                "    def __init__(self, input_vocab_size = input_vocab_size, embed_len = embed_len, dropout = dropout, device = device):\n",
                "        super(InputEmbedding, self).__init__()\n",
                "        self.input_vocab_size = input_vocab_size\n",
                "        self.embed_len = embed_len\n",
                "        self.dropout = dropout\n",
                "        self.device = device\n",
                "\n",
                "        self.firstEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
                "        self.secondEmbedding = nn.Embedding(self.input_vocab_size, self.embed_len)\n",
                "        self.dropoutLayer = nn.Dropout()\n",
                "\n",
                "    def forward(self, input):\n",
                "        first_embedding = self.firstEmbedding(input)\n",
                "        batch_size, seq_len = input.shape\n",
                "\n",
                "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(device)\n",
                "        positional_encoding = self.secondEmbedding(positions_vector)\n",
                "        return self.dropoutLayer(first_embedding + positional_encoding)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 170,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([8, 20, 512])"
                        ]
                    },
                    "execution_count": 170,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "input = torch.randint(10, (8,20)).to(device)\n",
                "\n",
                "embedding = InputEmbedding().to(device)\n",
                "out = embedding(input).to(device)\n",
                "out.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Building the Scaled Dot Product block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 171,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ScaledDotProduct(nn.Module):\n",
                "    def __init__(self, embed_len = embed_len, mask = None):\n",
                "        super(ScaledDotProduct,self).__init__()\n",
                "        self.embed_len = embed_len\n",
                "        self.mask = mask\n",
                "        self.dk = embed_len # dimension of keys and queries\n",
                "\n",
                "        self.softmax = nn.Softmax(dim= 3)\n",
                "\n",
                "    def forward(self, queries, keys, values):\n",
                "        compatibility = torch.matmul(queries, torch.transpose(keys,2,3))\n",
                "        compatibility = compatibility/math.sqrt(self.dk)\n",
                "\n",
                "        compatibility = self.softmax(compatibility)\n",
                "\n",
                "        # apply a mask for the decoder\n",
                "        if self.mask is not None:\n",
                "            compatibility = torch.tril(compatibility)\n",
                "\n",
                "\n",
                "        return torch.matmul(compatibility, torch.transpose(values, 1, 2))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Building MultiHeaded implementation block."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 173,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, num_heads = num_heads, embed_len = embed_len, batch_size= batch_size, mask = None):\n",
                "        super(MultiHeadAttention, self).__init__()\n",
                "        self.num_heads =num_heads\n",
                "        self.embed_len = embed_len\n",
                "        self.batch_size =  batch_size\n",
                "        self.mask = mask\n",
                "        self.head_length = int(self.embed_len/ self.num_heads)\n",
                "\n",
                "        self.q_in = self.v_in = self.k_in = self.embed_len\n",
                "\n",
                "        #Linear layers as input to the multihead attention\n",
                "        self.q_linear = nn.Linear(int(self.q_in), int(self.q_in))\n",
                "        self.k_linear = nn.Linear(int(self.k_in), int(self.k_in))\n",
                "        self.v_linear = nn.Linear(int(self.v_in), int(self.v_in))\n",
                "\n",
                "        #Activate mask for the decoder\n",
                "        if self.mask is not None:\n",
                "            self.attention = ScaledDotProduct(mask=True)\n",
                "        else:\n",
                "            self.attention =ScaledDotProduct()\n",
                "\n",
                "        self.output_linear = nn.Linear(self.q_in, self.q_in)\n",
                "\n",
                "    \n",
                "    def forward(self, queries, keys, values):\n",
                "\n",
                "        # queries shape = (8, 20, 512)\n",
                "        # we need to reshape (batch_size, seq-len, embed-len) to (batch_size, seq_len, num_heads, head_length)\n",
                "        # output should be reshaped into (8, 20, 8, 64)\n",
                "        #after transpose the output would become(8,8,20,64)\n",
                "        queries = self.q_linear(queries).reshape(\n",
                "            self.batch_size, -1, self.num_heads, self.head_length)\n",
                "        queries = queries.transpose(1,2)\n",
                "\n",
                "        #keys shape = (batch_size, num_heads, seq_len, head_length)\n",
                "        keys =self.k_linear(keys).reshape(\n",
                "            self.batch_size, -1, self.num_heads, self.head_length)\n",
                "        keys = keys.transpose(1,2)\n",
                "\n",
                "        # values shape = (batch_size, num_heads, seq_len, head_lengh)\n",
                "        values = self.v_linear(values).reshape(\n",
                "            self.batch_size, -1, self.num_heads, self.head_length)\n",
                "        # QK result dimension(batch_size, num_heads, seq_len) -> (8,8,20,20)\n",
                "        # QK matmul V -> (8,8,20,20)  matmul (8,8,20,64) -> (8,8,20,64)\n",
                "        # final shape should be (batch_size, seq_len, embed_len)\n",
                "        sdp_output = self.attention.forward(queries, keys, values).transpose(1,2).reshape(\n",
                "            self.batch_size, -1, self.num_heads*self.head_length\n",
                "        )\n",
                "\n",
                "        #output has size(8, 20,512)\n",
                "        return self.output_linear(sdp_output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 174,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderBlock(nn.Module):\n",
                "    def __init__(self,embed_len= embed_len, dropout =\n",
                "                 dropout):\n",
                "        super(EncoderBlock, self).__init__()\n",
                "        self.embed_len = embed_len\n",
                "        self.dropout =dropout\n",
                "        self.multihead =MultiHeadAttention()\n",
                "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
                "        self.secondNorm = nn.LayerNorm(self.embed_len)\n",
                "        self.dropoutLayer = nn.Dropout(p = self.dropout)\n",
                "\n",
                "        self.feedForward = nn.Sequential(\n",
                "            nn.Linear(self.embed_len, self.embed_len*4),\n",
                "            nn.ReLU(),            \n",
                "            nn.Linear(self.embed_len*4, self.embed_len*4)\n",
                "        )\n",
                "\n",
                "\n",
                "    def forward(self, queries, keys, values):\n",
                "        attention_output = self.multihead.forward(queries, keys, values)\n",
                "        attention_output = self.dropoutLayer(attention_output)\n",
                "        first_sublayer_output = self.firstNorm(attention_output +queries)\n",
                "\n",
                "        ff_output = self.feedForward(first_sublayer_output)\n",
                "        ff_output = self.dropoutLayer(ff_output)\n",
                "\n",
                "        return self.secondNorm(ff_output + first_sublayer_output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 179,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderBlock(nn.Module):\n",
                "    def __init__(self,embed_len= embed_len, dropout =\n",
                "                 dropout):\n",
                "        super(DecoderBlock, self).__init__()\n",
                "        self.embed_len = embed_len\n",
                "        self.dropout =dropout\n",
                "        self.multihead =MultiHeadAttention( mask= True)\n",
                "        self.firstNorm = nn.LayerNorm(self.embed_len)\n",
                "        self.dropoutLayer = nn.Dropout(p = self.dropout)\n",
                "\n",
                "        self.encoderBlock = EncoderBlock()\n",
                "\n",
                "    def forward(self, queries, keys, values):\n",
                "        masked_multihead_output = self.multihead.forward(queries, queries, queries)\n",
                "        masked_multihead_output = self.dropoutLayer(masked_multihead_output)\n",
                "        first_sublayer_output = self.firstNorm(masked_multihead_output +queries)\n",
                "\n",
                "        return self.encoderBlock(first_sublayer_output, keys, values)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "implement full transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 180,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(self, embed_len= embed_len, stack_len = stack_len, device= device, output_vocab_size=output_vocab_size):\n",
                "        super(Transformer, self).__init__()\n",
                "\n",
                "        self.embed_len= embed_len\n",
                "        self.stack_len = stack_len\n",
                "        self.device= device\n",
                "        self.output_vocab_size=output_vocab_size\n",
                "\n",
                "        self.embedding = InputEmbedding().to(self.device)\n",
                "        self.encstack = nn.ModuleList(EncoderBlock() for i in range(self.stack_len)).to(self.device)\n",
                "        self.decstack = nn.ModuleList(DecoderBlock() for i in range(self.stack_len)).to(self.device)\n",
                "\n",
                "        self.finalLinear = nn.Linear(self.embed_len, self.output_vocab_size).to(device)\n",
                "        self.softmax = nn.Softmax()\n",
                "\n",
                "    \n",
                "    def forward(self, test_input, test_output):\n",
                "        enc_output = self.embedding.forward(test_input)\n",
                "\n",
                "        for enc_layer in self.encstack:\n",
                "            enc_output = enc_layer.forward(enc_output,enc_output,enc_output)\n",
                "\n",
                "        dec_output = self.embedding(test_output)\n",
                "        for dec_layer in self.decstack:\n",
                "            dec_output = dec_layer.forward(dec_output,enc_output,enc_output)\n",
                "\n",
                "        final_output =self.finalLinear(dec_output)\n",
                "\n",
                "        return self.softmax(final_output)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 181,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 2",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[181], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m output_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m10\u001b[39m, (batch_size, \u001b[39m20\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m transformer \u001b[39m=\u001b[39m Transformer()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 5\u001b[0m transformer_output \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mforward(input_token, output_target)\n",
                        "Cell \u001b[1;32mIn[180], line 22\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, test_input, test_output)\u001b[0m\n\u001b[0;32m     19\u001b[0m enc_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mforward(test_input)\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m enc_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencstack:\n\u001b[1;32m---> 22\u001b[0m     enc_output \u001b[39m=\u001b[39m enc_layer\u001b[39m.\u001b[39;49mforward(enc_output,enc_output,enc_output)\n\u001b[0;32m     24\u001b[0m dec_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(test_output)\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m dec_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecstack:\n",
                        "Cell \u001b[1;32mIn[174], line 27\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, queries, keys, values)\u001b[0m\n\u001b[0;32m     24\u001b[0m ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedForward(first_sublayer_output)\n\u001b[0;32m     25\u001b[0m ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropoutLayer(ff_output)\n\u001b[1;32m---> 27\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecondNorm(ff_output \u001b[39m+\u001b[39;49m first_sublayer_output)\n",
                        "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (512) at non-singleton dimension 2"
                    ]
                }
            ],
            "source": [
                "input_token = torch.randint(10, (batch_size, 30)).to(device)\n",
                "output_target = torch.randint(10, (batch_size, 20)).to(device)\n",
                "\n",
                "transformer = Transformer().to(device)\n",
                "transformer_output = transformer.forward(input_token, output_target)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
